{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fdbbd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5404d0",
   "metadata": {},
   "source": [
    "### 데이터 shape 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7057fdc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 9)\n"
     ]
    }
   ],
   "source": [
    "dataset = np.loadtxt('./data_set/pima-indians-diabetes.csv', delimiter = ',')\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99d3684",
   "metadata": {},
   "source": [
    "### 데이터 value 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "957b0391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  6.    148.     72.    ...  33.6     0.627  50.   ]\n",
      " [  1.     85.     66.    ...  26.6     0.351  31.   ]\n",
      " [  8.    183.     64.    ...  23.3     0.672  32.   ]\n",
      " ...\n",
      " [  5.    121.     72.    ...  26.2     0.245  30.   ]\n",
      " [  1.    126.     60.    ...  30.1     0.349  47.   ]\n",
      " [  1.     93.     70.    ...  30.4     0.315  23.   ]]\n",
      "[1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1.\n",
      " 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0.\n",
      " 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0.\n",
      " 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0.\n",
      " 1. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0.\n",
      " 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0.\n",
      " 1. 1. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1.\n",
      " 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0.\n",
      " 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0.\n",
      " 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1.\n",
      " 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 0.\n",
      " 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1.\n",
      " 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0.\n",
      " 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0.\n",
      " 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0.\n",
      " 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1.\n",
      " 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1.\n",
      " 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1.\n",
      " 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "X = dataset[:, 0 : 8]\n",
    "Y = dataset[:, -1]\n",
    "\n",
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca48803",
   "metadata": {},
   "source": [
    "## Sequential 모델 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b877dcdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense1 (Dense)               (None, 12)                108       \n",
      "_________________________________________________________________\n",
      "dense2 (Dense)               (None, 8)                 104       \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 221\n",
      "Trainable params: 221\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential(name = 'Sequential')\n",
    "model.add(tf.keras.layers.Input(shape = (8, ), name = 'input'))\n",
    "model.add(tf.keras.layers.Dense(units = 12, activation = 'relu', name = 'dense1'))\n",
    "model.add(tf.keras.layers.Dense(units = 8, activation = 'relu', name = 'dense2'))\n",
    "model.add(tf.keras.layers.Dense(units = 1, activation = 'sigmoid', name = 'output'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "536ba08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.6384 - acc: 0.6576\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 0s 729us/step - loss: 0.6305 - acc: 0.6693\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 0s 711us/step - loss: 0.6362 - acc: 0.6562\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 0s 749us/step - loss: 0.6275 - acc: 0.6784\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 0s 681us/step - loss: 0.6226 - acc: 0.6784\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 0s 697us/step - loss: 0.6215 - acc: 0.6914\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 0s 775us/step - loss: 0.6178 - acc: 0.6810\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 0s 676us/step - loss: 0.6170 - acc: 0.6849\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 0s 687us/step - loss: 0.6141 - acc: 0.6992\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 0s 760us/step - loss: 0.6113 - acc: 0.6953\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 0s 804us/step - loss: 0.6110 - acc: 0.6927\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 0s 934us/step - loss: 0.6092 - acc: 0.6927\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 0s 704us/step - loss: 0.6103 - acc: 0.6914\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 0s 732us/step - loss: 0.6061 - acc: 0.7018\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 0s 918us/step - loss: 0.6026 - acc: 0.6979\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 0s 687us/step - loss: 0.6021 - acc: 0.6953\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 0s 699us/step - loss: 0.5978 - acc: 0.7057\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.5981 - acc: 0.6953\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 0s 687us/step - loss: 0.5959 - acc: 0.7096\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 0s 756us/step - loss: 0.5934 - acc: 0.7070\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 0s 781us/step - loss: 0.5916 - acc: 0.6927\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 0s 707us/step - loss: 0.5904 - acc: 0.7031\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 0s 830us/step - loss: 0.5875 - acc: 0.7018\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 0s 898us/step - loss: 0.5855 - acc: 0.6966\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 0s 711us/step - loss: 0.5874 - acc: 0.7044\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 0s 748us/step - loss: 0.5849 - acc: 0.6992\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 0s 793us/step - loss: 0.5778 - acc: 0.7109\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 0s 839us/step - loss: 0.5785 - acc: 0.7161\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 0s 713us/step - loss: 0.5777 - acc: 0.6966\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 0s 836us/step - loss: 0.5731 - acc: 0.7070\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 0s 917us/step - loss: 0.5728 - acc: 0.7096\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 0s 781us/step - loss: 0.5713 - acc: 0.7005\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 0s 825us/step - loss: 0.5678 - acc: 0.7161\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 0s 928us/step - loss: 0.5670 - acc: 0.7122\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.5664 - acc: 0.7135\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.5655 - acc: 0.7057\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 0s 901us/step - loss: 0.5629 - acc: 0.7096\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.5628 - acc: 0.7148\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.5601 - acc: 0.7070\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.5609 - acc: 0.7148\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.5615 - acc: 0.7161\n",
      "Epoch 42/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.5629 - acc: 0.7161\n",
      "Epoch 43/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.5580 - acc: 0.7227\n",
      "Epoch 44/100\n",
      "11/11 [==============================] - 0s 937us/step - loss: 0.5596 - acc: 0.7096\n",
      "Epoch 45/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.5546 - acc: 0.7253\n",
      "Epoch 46/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.5544 - acc: 0.7292\n",
      "Epoch 47/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.5527 - acc: 0.7214\n",
      "Epoch 48/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.5508 - acc: 0.7266\n",
      "Epoch 49/100\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.5526 - acc: 0.7214\n",
      "Epoch 50/100\n",
      "11/11 [==============================] - 0s 928us/step - loss: 0.5474 - acc: 0.7266\n",
      "Epoch 51/100\n",
      "11/11 [==============================] - 0s 914us/step - loss: 0.5478 - acc: 0.7214\n",
      "Epoch 52/100\n",
      "11/11 [==============================] - 0s 851us/step - loss: 0.5519 - acc: 0.7214\n",
      "Epoch 53/100\n",
      "11/11 [==============================] - 0s 807us/step - loss: 0.5444 - acc: 0.7279\n",
      "Epoch 54/100\n",
      "11/11 [==============================] - 0s 768us/step - loss: 0.5453 - acc: 0.7318\n",
      "Epoch 55/100\n",
      "11/11 [==============================] - 0s 673us/step - loss: 0.5431 - acc: 0.7279\n",
      "Epoch 56/100\n",
      "11/11 [==============================] - 0s 759us/step - loss: 0.5422 - acc: 0.7305\n",
      "Epoch 57/100\n",
      "11/11 [==============================] - 0s 704us/step - loss: 0.5442 - acc: 0.7305\n",
      "Epoch 58/100\n",
      "11/11 [==============================] - 0s 731us/step - loss: 0.5484 - acc: 0.7148\n",
      "Epoch 59/100\n",
      "11/11 [==============================] - 0s 721us/step - loss: 0.5406 - acc: 0.7435\n",
      "Epoch 60/100\n",
      "11/11 [==============================] - 0s 671us/step - loss: 0.5409 - acc: 0.7305\n",
      "Epoch 61/100\n",
      "11/11 [==============================] - 0s 764us/step - loss: 0.5407 - acc: 0.7279\n",
      "Epoch 62/100\n",
      "11/11 [==============================] - 0s 672us/step - loss: 0.5433 - acc: 0.7305\n",
      "Epoch 63/100\n",
      "11/11 [==============================] - 0s 794us/step - loss: 0.5385 - acc: 0.7344\n",
      "Epoch 64/100\n",
      "11/11 [==============================] - 0s 756us/step - loss: 0.5417 - acc: 0.7292\n",
      "Epoch 65/100\n",
      "11/11 [==============================] - 0s 687us/step - loss: 0.5541 - acc: 0.7188\n",
      "Epoch 66/100\n",
      "11/11 [==============================] - 0s 750us/step - loss: 0.5442 - acc: 0.7279\n",
      "Epoch 67/100\n",
      "11/11 [==============================] - 0s 681us/step - loss: 0.5390 - acc: 0.7135\n",
      "Epoch 68/100\n",
      "11/11 [==============================] - 0s 747us/step - loss: 0.5448 - acc: 0.7357\n",
      "Epoch 69/100\n",
      "11/11 [==============================] - 0s 701us/step - loss: 0.5494 - acc: 0.7214\n",
      "Epoch 70/100\n",
      "11/11 [==============================] - 0s 771us/step - loss: 0.5437 - acc: 0.7305\n",
      "Epoch 71/100\n",
      "11/11 [==============================] - 0s 744us/step - loss: 0.5374 - acc: 0.7253\n",
      "Epoch 72/100\n",
      "11/11 [==============================] - 0s 661us/step - loss: 0.5493 - acc: 0.7201\n",
      "Epoch 73/100\n",
      "11/11 [==============================] - 0s 757us/step - loss: 0.5390 - acc: 0.7305\n",
      "Epoch 74/100\n",
      "11/11 [==============================] - 0s 678us/step - loss: 0.5332 - acc: 0.7396\n",
      "Epoch 75/100\n",
      "11/11 [==============================] - 0s 728us/step - loss: 0.5319 - acc: 0.7487\n",
      "Epoch 76/100\n",
      "11/11 [==============================] - 0s 679us/step - loss: 0.5320 - acc: 0.7344\n",
      "Epoch 77/100\n",
      "11/11 [==============================] - 0s 722us/step - loss: 0.5330 - acc: 0.7357\n",
      "Epoch 78/100\n",
      "11/11 [==============================] - 0s 732us/step - loss: 0.5327 - acc: 0.7409\n",
      "Epoch 79/100\n",
      "11/11 [==============================] - 0s 695us/step - loss: 0.5308 - acc: 0.7435\n",
      "Epoch 80/100\n",
      "11/11 [==============================] - 0s 730us/step - loss: 0.5323 - acc: 0.7526\n",
      "Epoch 81/100\n",
      "11/11 [==============================] - 0s 673us/step - loss: 0.5321 - acc: 0.7461\n",
      "Epoch 82/100\n",
      "11/11 [==============================] - 0s 711us/step - loss: 0.5287 - acc: 0.7435\n",
      "Epoch 83/100\n",
      "11/11 [==============================] - 0s 667us/step - loss: 0.5293 - acc: 0.7383\n",
      "Epoch 84/100\n",
      "11/11 [==============================] - 0s 736us/step - loss: 0.5288 - acc: 0.7461\n",
      "Epoch 85/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 870us/step - loss: 0.5282 - acc: 0.7500\n",
      "Epoch 86/100\n",
      "11/11 [==============================] - 0s 713us/step - loss: 0.5267 - acc: 0.7461\n",
      "Epoch 87/100\n",
      "11/11 [==============================] - 0s 774us/step - loss: 0.5267 - acc: 0.7474\n",
      "Epoch 88/100\n",
      "11/11 [==============================] - 0s 741us/step - loss: 0.5246 - acc: 0.7513\n",
      "Epoch 89/100\n",
      "11/11 [==============================] - 0s 678us/step - loss: 0.5262 - acc: 0.7474\n",
      "Epoch 90/100\n",
      "11/11 [==============================] - 0s 772us/step - loss: 0.5372 - acc: 0.7448\n",
      "Epoch 91/100\n",
      "11/11 [==============================] - 0s 691us/step - loss: 0.5343 - acc: 0.7474\n",
      "Epoch 92/100\n",
      "11/11 [==============================] - 0s 705us/step - loss: 0.5325 - acc: 0.7409\n",
      "Epoch 93/100\n",
      "11/11 [==============================] - 0s 690us/step - loss: 0.5281 - acc: 0.7357\n",
      "Epoch 94/100\n",
      "11/11 [==============================] - 0s 707us/step - loss: 0.5305 - acc: 0.7526\n",
      "Epoch 95/100\n",
      "11/11 [==============================] - 0s 683us/step - loss: 0.5210 - acc: 0.7487\n",
      "Epoch 96/100\n",
      "11/11 [==============================] - 0s 691us/step - loss: 0.5222 - acc: 0.7474\n",
      "Epoch 97/100\n",
      "11/11 [==============================] - 0s 696us/step - loss: 0.5216 - acc: 0.7487\n",
      "Epoch 98/100\n",
      "11/11 [==============================] - 0s 657us/step - loss: 0.5268 - acc: 0.7435\n",
      "Epoch 99/100\n",
      "11/11 [==============================] - 0s 712us/step - loss: 0.5241 - acc: 0.7513\n",
      "Epoch 100/100\n",
      "11/11 [==============================] - 0s 681us/step - loss: 0.5244 - acc: 0.7565\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f87456beb70>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss = 'binary_crossentropy', # 결과가 0 또는 1로 두개이기때문에 // 다중일 경우 categorical_crossentropy\n",
    "              optimizer = 'adam', metrics = ['acc']\n",
    "             )\n",
    "\n",
    "model.fit(X, Y, epochs = 100, batch_size = 70)\n",
    "## overfitting 확인은 callback 함수 사용해야 한다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadae6ad",
   "metadata": {},
   "source": [
    "## Funtional API 모델 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a2abbaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Functional_API\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 8)]               0         \n",
      "_________________________________________________________________\n",
      "dense1 (Dense)               (None, 12)                108       \n",
      "_________________________________________________________________\n",
      "dense2 (Dense)               (None, 8)                 104       \n",
      "_________________________________________________________________\n",
      "outputs (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 221\n",
      "Trainable params: 221\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.keras.layers.Input(shape = (8, ), name = 'input')\n",
    "dense1 = tf.keras.layers.Dense(units = 12, activation = 'relu', name = 'dense1')(inputs)\n",
    "dense2 = tf.keras.layers.Dense(units = 8, activation = 'relu', name = 'dense2')(dense1)\n",
    "outputs = tf.keras.layers.Dense(units = 1, activation = 'sigmoid', name = 'outputs')(dense2)\n",
    "\n",
    "model = tf.keras.Model(inputs = inputs, outputs = outputs, name = 'Functional_API')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4927bf",
   "metadata": {},
   "source": [
    "## Subclassing  모델 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "52ffc6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model) :\n",
    "    def __init__(self, hidden1, hidden2, output_layers) :\n",
    "        super(MyModel, self).__init__(name = 'MyModel')\n",
    "        self.dense1 = tf.keras.layers.Dense(units = hidden1, activation = 'relu', name = 'dense1')\n",
    "        self.dense2 = tf.keras.layers.Dense(units = hidden2, activation = 'relu', name = 'dense2')\n",
    "        self.output_layer = tf.keras.layers.Dense(units = output_layers, activation = 'sigmoid', name = 'output_layer')\n",
    "        \n",
    "    def call(self, inputs) :\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dense2(x)\n",
    "        \n",
    "        return self.output_layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9696db",
   "metadata": {},
   "source": [
    "## 모델 저장 및 early_stopping( 10 회 이상 바뀐게 없으면 종료)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8f095c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath = './model_save/{epoch:2d}_{val_loss:4f}.hdf5', monitor = 'val_loss', verbose = 1 , \n",
    "                                               save_best_only = True, save_weights_only = True)\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd6142d",
   "metadata": {},
   "source": [
    "## 모델 학습 진행\n",
    "### history 에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e0028a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 12.4632 - acc: 0.4609 - val_loss: 8.9061 - val_acc: 0.5390\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 8.90608, saving model to ./model_save/ 1_8.906078.hdf5\n",
      "Epoch 2/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 7.6770 - acc: 0.5049 - val_loss: 5.0376 - val_acc: 0.5844\n",
      "\n",
      "Epoch 00002: val_loss improved from 8.90608 to 5.03755, saving model to ./model_save/ 2_5.037552.hdf5\n",
      "Epoch 3/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 4.3143 - acc: 0.5749 - val_loss: 3.3638 - val_acc: 0.5519\n",
      "\n",
      "Epoch 00003: val_loss improved from 5.03755 to 3.36378, saving model to ./model_save/ 3_3.363777.hdf5\n",
      "Epoch 4/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3.2428 - acc: 0.6368 - val_loss: 3.4895 - val_acc: 0.5974\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 3.36378\n",
      "Epoch 5/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3.0406 - acc: 0.6645 - val_loss: 3.2301 - val_acc: 0.6169\n",
      "\n",
      "Epoch 00005: val_loss improved from 3.36378 to 3.23011, saving model to ./model_save/ 5_3.230108.hdf5\n",
      "Epoch 6/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 2.7678 - acc: 0.6759 - val_loss: 2.8027 - val_acc: 0.5974\n",
      "\n",
      "Epoch 00006: val_loss improved from 3.23011 to 2.80272, saving model to ./model_save/ 6_2.802717.hdf5\n",
      "Epoch 7/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 2.5025 - acc: 0.6564 - val_loss: 2.4908 - val_acc: 0.5714\n",
      "\n",
      "Epoch 00007: val_loss improved from 2.80272 to 2.49076, saving model to ./model_save/ 7_2.490760.hdf5\n",
      "Epoch 8/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 2.2931 - acc: 0.6531 - val_loss: 2.3126 - val_acc: 0.5779\n",
      "\n",
      "Epoch 00008: val_loss improved from 2.49076 to 2.31259, saving model to ./model_save/ 8_2.312589.hdf5\n",
      "Epoch 9/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 2.1715 - acc: 0.6417 - val_loss: 2.1358 - val_acc: 0.5584\n",
      "\n",
      "Epoch 00009: val_loss improved from 2.31259 to 2.13584, saving model to ./model_save/ 9_2.135836.hdf5\n",
      "Epoch 10/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 2.0036 - acc: 0.6531 - val_loss: 2.0074 - val_acc: 0.6169\n",
      "\n",
      "Epoch 00010: val_loss improved from 2.13584 to 2.00738, saving model to ./model_save/10_2.007375.hdf5\n",
      "Epoch 11/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 1.8948 - acc: 0.6384 - val_loss: 1.8785 - val_acc: 0.5974\n",
      "\n",
      "Epoch 00011: val_loss improved from 2.00738 to 1.87852, saving model to ./model_save/11_1.878522.hdf5\n",
      "Epoch 12/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 1.7661 - acc: 0.6401 - val_loss: 1.7768 - val_acc: 0.6234\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.87852 to 1.77676, saving model to ./model_save/12_1.776761.hdf5\n",
      "Epoch 13/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 1.6676 - acc: 0.6450 - val_loss: 1.6743 - val_acc: 0.6299\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.77676 to 1.67426, saving model to ./model_save/13_1.674256.hdf5\n",
      "Epoch 14/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 1.6219 - acc: 0.6319 - val_loss: 1.6671 - val_acc: 0.6039\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.67426 to 1.66709, saving model to ./model_save/14_1.667085.hdf5\n",
      "Epoch 15/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 1.5387 - acc: 0.6303 - val_loss: 1.6031 - val_acc: 0.6169\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.66709 to 1.60312, saving model to ./model_save/15_1.603119.hdf5\n",
      "Epoch 16/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 1.4751 - acc: 0.6270 - val_loss: 1.5797 - val_acc: 0.5974\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.60312 to 1.57968, saving model to ./model_save/16_1.579677.hdf5\n",
      "Epoch 17/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 1.4354 - acc: 0.6368 - val_loss: 1.5137 - val_acc: 0.6169\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.57968 to 1.51366, saving model to ./model_save/17_1.513658.hdf5\n",
      "Epoch 18/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 1.4048 - acc: 0.6254 - val_loss: 1.4949 - val_acc: 0.6039\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.51366 to 1.49495, saving model to ./model_save/18_1.494948.hdf5\n",
      "Epoch 19/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 1.3606 - acc: 0.6319 - val_loss: 1.4341 - val_acc: 0.6104\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.49495 to 1.43414, saving model to ./model_save/19_1.434136.hdf5\n",
      "Epoch 20/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 1.3314 - acc: 0.6384 - val_loss: 1.3959 - val_acc: 0.6299\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.43414 to 1.39593, saving model to ./model_save/20_1.395931.hdf5\n",
      "Epoch 21/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 1.3245 - acc: 0.6270 - val_loss: 1.3738 - val_acc: 0.6039\n",
      "\n",
      "Epoch 00021: val_loss improved from 1.39593 to 1.37382, saving model to ./model_save/21_1.373819.hdf5\n",
      "Epoch 22/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 1.2670 - acc: 0.6384 - val_loss: 1.3053 - val_acc: 0.6169\n",
      "\n",
      "Epoch 00022: val_loss improved from 1.37382 to 1.30534, saving model to ./model_save/22_1.305336.hdf5\n",
      "Epoch 23/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 1.2414 - acc: 0.6564 - val_loss: 1.2789 - val_acc: 0.6299\n",
      "\n",
      "Epoch 00023: val_loss improved from 1.30534 to 1.27890, saving model to ./model_save/23_1.278903.hdf5\n",
      "Epoch 24/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 1.1932 - acc: 0.6384 - val_loss: 1.2328 - val_acc: 0.6234\n",
      "\n",
      "Epoch 00024: val_loss improved from 1.27890 to 1.23282, saving model to ./model_save/24_1.232820.hdf5\n",
      "Epoch 25/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 1.1734 - acc: 0.6466 - val_loss: 1.2006 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00025: val_loss improved from 1.23282 to 1.20062, saving model to ./model_save/25_1.200625.hdf5\n",
      "Epoch 26/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 1.1321 - acc: 0.6368 - val_loss: 1.1929 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00026: val_loss improved from 1.20062 to 1.19293, saving model to ./model_save/26_1.192932.hdf5\n",
      "Epoch 27/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 1.1136 - acc: 0.6531 - val_loss: 1.1379 - val_acc: 0.6429\n",
      "\n",
      "Epoch 00027: val_loss improved from 1.19293 to 1.13793, saving model to ./model_save/27_1.137934.hdf5\n",
      "Epoch 28/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 1.0997 - acc: 0.6482 - val_loss: 1.1510 - val_acc: 0.6429\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 1.13793\n",
      "Epoch 29/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 1.0840 - acc: 0.6450 - val_loss: 1.1581 - val_acc: 0.5974\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 1.13793\n",
      "Epoch 30/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 1.0722 - acc: 0.6661 - val_loss: 1.0575 - val_acc: 0.6429\n",
      "\n",
      "Epoch 00030: val_loss improved from 1.13793 to 1.05754, saving model to ./model_save/30_1.057540.hdf5\n",
      "Epoch 31/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 1.0502 - acc: 0.6352 - val_loss: 1.0657 - val_acc: 0.6429\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 1.05754\n",
      "Epoch 32/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 1.0687 - acc: 0.6482 - val_loss: 1.0509 - val_acc: 0.6169\n",
      "\n",
      "Epoch 00032: val_loss improved from 1.05754 to 1.05087, saving model to ./model_save/32_1.050866.hdf5\n",
      "Epoch 33/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.9948 - acc: 0.6547 - val_loss: 1.0564 - val_acc: 0.6494\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 1.05087\n",
      "Epoch 34/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.9665 - acc: 0.6792 - val_loss: 0.9497 - val_acc: 0.6494\n",
      "\n",
      "Epoch 00034: val_loss improved from 1.05087 to 0.94968, saving model to ./model_save/34_0.949677.hdf5\n",
      "Epoch 35/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.9453 - acc: 0.6678 - val_loss: 0.9390 - val_acc: 0.6299\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.94968 to 0.93900, saving model to ./model_save/35_0.938995.hdf5\n",
      "Epoch 36/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.9415 - acc: 0.6580 - val_loss: 0.9451 - val_acc: 0.6494\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.93900\n",
      "Epoch 37/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.9127 - acc: 0.6596 - val_loss: 0.9132 - val_acc: 0.6234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00037: val_loss improved from 0.93900 to 0.91317, saving model to ./model_save/37_0.913169.hdf5\n",
      "Epoch 38/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.9008 - acc: 0.6564 - val_loss: 0.8853 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.91317 to 0.88527, saving model to ./model_save/38_0.885268.hdf5\n",
      "Epoch 39/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.8694 - acc: 0.6792 - val_loss: 0.8639 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.88527 to 0.86391, saving model to ./model_save/39_0.863913.hdf5\n",
      "Epoch 40/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.8635 - acc: 0.6726 - val_loss: 0.8550 - val_acc: 0.6623\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.86391 to 0.85501, saving model to ./model_save/40_0.855008.hdf5\n",
      "Epoch 41/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.8526 - acc: 0.6726 - val_loss: 0.9011 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.85501\n",
      "Epoch 42/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.8528 - acc: 0.6726 - val_loss: 0.8344 - val_acc: 0.6429\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.85501 to 0.83437, saving model to ./model_save/42_0.834367.hdf5\n",
      "Epoch 43/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.8441 - acc: 0.6694 - val_loss: 0.8147 - val_acc: 0.6429\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.83437 to 0.81468, saving model to ./model_save/43_0.814677.hdf5\n",
      "Epoch 44/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.8128 - acc: 0.6792 - val_loss: 0.7928 - val_acc: 0.6234\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.81468 to 0.79284, saving model to ./model_save/44_0.792842.hdf5\n",
      "Epoch 45/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.8090 - acc: 0.6857 - val_loss: 0.7851 - val_acc: 0.6688\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.79284 to 0.78511, saving model to ./model_save/45_0.785108.hdf5\n",
      "Epoch 46/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.8196 - acc: 0.6824 - val_loss: 0.7750 - val_acc: 0.6429\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.78511 to 0.77497, saving model to ./model_save/46_0.774966.hdf5\n",
      "Epoch 47/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.7850 - acc: 0.7020 - val_loss: 0.7902 - val_acc: 0.6494\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.77497\n",
      "Epoch 48/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.7840 - acc: 0.7085 - val_loss: 0.7983 - val_acc: 0.6299\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.77497\n",
      "Epoch 49/100\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.7881 - acc: 0.6857 - val_loss: 0.7511 - val_acc: 0.6429\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.77497 to 0.75105, saving model to ./model_save/49_0.751054.hdf5\n",
      "Epoch 50/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.7625 - acc: 0.6987 - val_loss: 0.7618 - val_acc: 0.6429\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.75105\n",
      "Epoch 51/100\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.7570 - acc: 0.6808 - val_loss: 0.7401 - val_acc: 0.6429\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.75105 to 0.74015, saving model to ./model_save/51_0.740146.hdf5\n",
      "Epoch 52/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.7574 - acc: 0.7085 - val_loss: 0.7257 - val_acc: 0.6623\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.74015 to 0.72565, saving model to ./model_save/52_0.725652.hdf5\n",
      "Epoch 53/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.7437 - acc: 0.7166 - val_loss: 0.7541 - val_acc: 0.6558\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.72565\n",
      "Epoch 54/100\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.7556 - acc: 0.6906 - val_loss: 0.7419 - val_acc: 0.6429\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.72565\n",
      "Epoch 55/100\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.7296 - acc: 0.7166 - val_loss: 0.7262 - val_acc: 0.6558\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.72565\n",
      "Epoch 56/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.7364 - acc: 0.6987 - val_loss: 0.7122 - val_acc: 0.6494\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.72565 to 0.71223, saving model to ./model_save/56_0.712228.hdf5\n",
      "Epoch 57/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.7356 - acc: 0.7068 - val_loss: 0.7204 - val_acc: 0.6558\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.71223\n",
      "Epoch 58/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.7207 - acc: 0.7199 - val_loss: 0.6988 - val_acc: 0.6688\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.71223 to 0.69883, saving model to ./model_save/58_0.698831.hdf5\n",
      "Epoch 59/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.7145 - acc: 0.7150 - val_loss: 0.7035 - val_acc: 0.6753\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.69883\n",
      "Epoch 60/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.7128 - acc: 0.7101 - val_loss: 0.6844 - val_acc: 0.6883\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.69883 to 0.68443, saving model to ./model_save/60_0.684434.hdf5\n",
      "Epoch 61/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.7037 - acc: 0.7199 - val_loss: 0.7011 - val_acc: 0.6688\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.68443\n",
      "Epoch 62/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.7196 - acc: 0.6922 - val_loss: 0.7036 - val_acc: 0.6494\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.68443\n",
      "Epoch 63/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.6980 - acc: 0.7280 - val_loss: 0.6871 - val_acc: 0.6818\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.68443\n",
      "Epoch 64/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.6918 - acc: 0.7248 - val_loss: 0.6747 - val_acc: 0.6558\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.68443 to 0.67469, saving model to ./model_save/64_0.674694.hdf5\n",
      "Epoch 65/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.6902 - acc: 0.7231 - val_loss: 0.6828 - val_acc: 0.6818\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.67469\n",
      "Epoch 66/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.6928 - acc: 0.6922 - val_loss: 0.6985 - val_acc: 0.6558\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.67469\n",
      "Epoch 67/100\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.7051 - acc: 0.7036 - val_loss: 0.6843 - val_acc: 0.6623\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.67469\n",
      "Epoch 68/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.7123 - acc: 0.6906 - val_loss: 0.7025 - val_acc: 0.6623\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.67469\n",
      "Epoch 69/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.7042 - acc: 0.7101 - val_loss: 0.6859 - val_acc: 0.6753\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.67469\n",
      "Epoch 70/100\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.6789 - acc: 0.7117 - val_loss: 0.6560 - val_acc: 0.6883\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.67469 to 0.65603, saving model to ./model_save/70_0.656035.hdf5\n",
      "Epoch 71/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.6835 - acc: 0.7117 - val_loss: 0.6753 - val_acc: 0.6818\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.65603\n",
      "Epoch 72/100\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.6922 - acc: 0.7020 - val_loss: 0.6647 - val_acc: 0.6883\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.65603\n",
      "Epoch 73/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.6796 - acc: 0.7280 - val_loss: 0.6550 - val_acc: 0.6883\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.65603 to 0.65504, saving model to ./model_save/73_0.655044.hdf5\n",
      "Epoch 74/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.6632 - acc: 0.7329 - val_loss: 0.6489 - val_acc: 0.6948\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.65504 to 0.64893, saving model to ./model_save/74_0.648927.hdf5\n",
      "Epoch 75/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.6654 - acc: 0.7280 - val_loss: 0.6525 - val_acc: 0.6818\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.64893\n",
      "Epoch 76/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.6603 - acc: 0.7199 - val_loss: 0.6435 - val_acc: 0.6883\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.64893 to 0.64349, saving model to ./model_save/76_0.643485.hdf5\n",
      "Epoch 77/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 3ms/step - loss: 0.6577 - acc: 0.7362 - val_loss: 0.6503 - val_acc: 0.6688\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.64349\n",
      "Epoch 78/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.6540 - acc: 0.7443 - val_loss: 0.6412 - val_acc: 0.6948\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.64349 to 0.64119, saving model to ./model_save/78_0.641192.hdf5\n",
      "Epoch 79/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.6777 - acc: 0.7068 - val_loss: 0.7368 - val_acc: 0.6753\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.64119\n",
      "Epoch 80/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.7602 - acc: 0.6450 - val_loss: 0.6511 - val_acc: 0.6818\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.64119\n",
      "Epoch 81/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.7041 - acc: 0.6971 - val_loss: 0.6770 - val_acc: 0.6688\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.64119\n",
      "Epoch 82/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.6781 - acc: 0.7150 - val_loss: 0.6657 - val_acc: 0.6623\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.64119\n",
      "Epoch 83/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.6956 - acc: 0.6792 - val_loss: 0.6326 - val_acc: 0.6948\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.64119 to 0.63262, saving model to ./model_save/83_0.632621.hdf5\n",
      "Epoch 84/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.6677 - acc: 0.7117 - val_loss: 0.6458 - val_acc: 0.6948\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.63262\n",
      "Epoch 85/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.6503 - acc: 0.7264 - val_loss: 0.6497 - val_acc: 0.6494\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.63262\n",
      "Epoch 86/100\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.6464 - acc: 0.7280 - val_loss: 0.6432 - val_acc: 0.6753\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.63262\n",
      "Epoch 87/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.6596 - acc: 0.7085 - val_loss: 0.6938 - val_acc: 0.6883\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.63262\n",
      "Epoch 88/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.6725 - acc: 0.7117 - val_loss: 0.6377 - val_acc: 0.6818\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.63262\n",
      "Epoch 89/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.6399 - acc: 0.7362 - val_loss: 0.7027 - val_acc: 0.6234\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.63262\n",
      "Epoch 90/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.6660 - acc: 0.7052 - val_loss: 0.7370 - val_acc: 0.6753\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.63262\n",
      "Epoch 91/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.6805 - acc: 0.7101 - val_loss: 0.6343 - val_acc: 0.6818\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.63262\n",
      "Epoch 92/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.6443 - acc: 0.7296 - val_loss: 0.6434 - val_acc: 0.6753\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.63262\n",
      "Epoch 93/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.6306 - acc: 0.7264 - val_loss: 0.6246 - val_acc: 0.6818\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.63262 to 0.62457, saving model to ./model_save/93_0.624569.hdf5\n",
      "Epoch 94/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.6361 - acc: 0.7068 - val_loss: 0.6251 - val_acc: 0.6948\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.62457\n",
      "Epoch 95/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.6420 - acc: 0.7215 - val_loss: 0.6158 - val_acc: 0.6818\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.62457 to 0.61582, saving model to ./model_save/95_0.615825.hdf5\n",
      "Epoch 96/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.6255 - acc: 0.7410 - val_loss: 0.6271 - val_acc: 0.7208\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.61582\n",
      "Epoch 97/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.6265 - acc: 0.7199 - val_loss: 0.6073 - val_acc: 0.6753\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.61582 to 0.60735, saving model to ./model_save/97_0.607345.hdf5\n",
      "Epoch 98/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.6196 - acc: 0.7296 - val_loss: 0.6072 - val_acc: 0.6948\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.60735 to 0.60724, saving model to ./model_save/98_0.607241.hdf5\n",
      "Epoch 99/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.6172 - acc: 0.7296 - val_loss: 0.6103 - val_acc: 0.6688\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.60724\n",
      "Epoch 100/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.6233 - acc: 0.7264 - val_loss: 0.6147 - val_acc: 0.7013\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.60724\n",
      "Model: \"MyModel\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense1 (Dense)               multiple                  108       \n",
      "_________________________________________________________________\n",
      "dense2 (Dense)               multiple                  104       \n",
      "_________________________________________________________________\n",
      "output_layer (Dense)         multiple                  9         \n",
      "=================================================================\n",
      "Total params: 221\n",
      "Trainable params: 221\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_sub = MyModel(12, 8 , 1)\n",
    "\n",
    "model_sub.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['acc'])\n",
    "\n",
    "history = model_sub.fit(X, Y, epochs = 100, batch_size = 70, validation_split = 0.2, callbacks = [checkpoint, early_stopping])\n",
    "\n",
    "model_sub.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a8f80a9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7207792401313782"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(history.history['val_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "277b2a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.3.4-cp36-cp36m-macosx_10_9_x86_64.whl (8.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 8.5 MB 5.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.1 in /Users/kimhyunjo/opt/anaconda3/envs/nbkim/lib/python3.6/site-packages (from matplotlib) (2.8.2)\n",
      "Collecting pillow>=6.2.0\n",
      "  Downloading Pillow-8.4.0-cp36-cp36m-macosx_10_10_x86_64.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 22.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.3.1-cp36-cp36m-macosx_10_9_x86_64.whl (61 kB)\n",
      "\u001b[K     |████████████████████████████████| 61 kB 12.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cycler>=0.10\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /Users/kimhyunjo/opt/anaconda3/envs/nbkim/lib/python3.6/site-packages (from matplotlib) (3.0.4)\n",
      "Requirement already satisfied: numpy>=1.15 in /Users/kimhyunjo/opt/anaconda3/envs/nbkim/lib/python3.6/site-packages (from matplotlib) (1.19.5)\n",
      "Requirement already satisfied: six>=1.5 in /Users/kimhyunjo/opt/anaconda3/envs/nbkim/lib/python3.6/site-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n",
      "Installing collected packages: pillow, kiwisolver, cycler, matplotlib\n",
      "Successfully installed cycler-0.11.0 kiwisolver-1.3.1 matplotlib-3.3.4 pillow-8.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fd6c2871",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize = (10, 5))\n",
    "\n",
    "plt.plot(history.history['val_loss'], '-r', label = 'Validation loss')\n",
    "plt.plot(history.history['val_acc'], '-b', label = 'Validation acc')\n",
    "\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
